{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1- RAG\n",
    "using openai API and PineconeDB API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"8d156735-e53a-48ef-a158-068d20894e58\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone client\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "environment = \"us-east1-gcp\" \n",
    "pc = Pinecone(api_key=api_key, environment=environment)\n",
    "\n",
    "# Define index name and specifications\n",
    "index_name = \"task1-yardstick\"\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")  \n",
    "\n",
    "# Check if the index exists and create it if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(index_name, dimension=768, metric=\"cosine\", spec=spec)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this approach have used \"text-embedding-ada-002\" for text to embedding generation and \"gpt-3.5-turbo\" for response generation but I had to face \"RateLimitError\" (as shown below) because I am in free plan of openai API. I tried for lower tokens document/sentences also but I was unable to use openaiAPI. So to resolve this I have used another approach at the end using Sentence Transformers locally. \n",
    "\n",
    "#### Note\n",
    " Check second Approach using sentence transformers for succeful completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper FUnctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "\n",
    "# OpenAI setup\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# Initialize PCA for down-sampling\n",
    "pca = PCA(n_components=768)\n",
    "pca_fitted = False  # Ensure that PCA is fitted only once\n",
    "\n",
    "\n",
    "def search_similar_contexts(query_embedding: List[float], top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Search for similar contexts in the Pinecone index.\"\"\"\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    return [{\"id\": match[\"id\"], \"score\": match[\"score\"], \"context\": match[\"metadata\"][\"text\"]} for match in results[\"matches\"]]\n",
    "\n",
    "# Initialize PCA for down-sampling\n",
    "pca = PCA(n_components=768)\n",
    "pca_fitted = False  # Ensure that PCA is fitted only once\n",
    "\n",
    "def get_openai_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get the embedding for a query using OpenAI's API and down-sample it to 768 dimensions.\"\"\"\n",
    "    global pca_fitted\n",
    "    response = client.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    embedding = response.data[0].embedding\n",
    "\n",
    "    # Fit PCA to the first embedding and transform for down-sampling\n",
    "    if not pca_fitted:\n",
    "        pca.fit([embedding])  # Fit PCA using the first embedding\n",
    "        pca_fitted = True\n",
    "    downsampled_embedding = pca.transform([embedding])[0]  # Down-sample to 768 dimensions\n",
    "    return downsampled_embedding.tolist()\n",
    "\n",
    "def generate_answer(query: str, contexts: List[Dict]) -> str:\n",
    "    \"\"\"Generate an answer using OpenAI's API with retrieved contexts.\"\"\"\n",
    "    prompt = f\"Answer the following question based on the given contexts:\\n\\nQuestion: {query}\\n\\nContexts:\\n\"\n",
    "    for ctx in contexts:\n",
    "        prompt += f\"- {ctx['context']}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for a business.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populating pinecone index with smaller sentences to reduce tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_index(data: List[Dict[str, str]]):\n",
    "    \"\"\"Populate Pinecone index using Sentence-Transformers embeddings.\"\"\"\n",
    "    batch_size = 100  # Adjust based on your Pinecone plan and rate limits\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        vectors = []\n",
    "        for item in batch:\n",
    "            embedding = get_openai_embedding(item['text'])\n",
    "            vectors.append({\n",
    "                \"id\": item['id'],\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": {\"text\": item['text']}\n",
    "            })\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"Uploaded {len(vectors)} vectors to Pinecone\")\n",
    "        time.sleep(2)  # Add a small delay to avoid API rate limits\n",
    "\n",
    "\n",
    "# Sample data to populate Pinecone index\n",
    "sample_data = [\n",
    "    {\"id\": \"product1\", \"text\": \"Our main product is the SuperWidget 3000, a revolutionary device that increases productivity by 200%.\"},\n",
    "    {\"id\": \"product2\", \"text\": \"The MegaGadget Pro is our premium offering, designed for professional users who need advanced features.\"},\n",
    "    {\"id\": \"service1\", \"text\": \"We offer 24/7 customer support for all our products, ensuring our customers always have assistance when they need it.\"},\n",
    "    {\"id\": \"about1\", \"text\": \"Our company, TechInnovators Inc., was founded in 2010 with the mission to create cutting-edge technology solutions.\"},\n",
    "    {\"id\": \"vision1\", \"text\": \"Our vision is to become the global leader in innovative technology products by 2030.\"},\n",
    "    # Add more items as needed\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 5 vectors to Pinecone\n",
      "Index population completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Populate the index\n",
    "try:\n",
    "    populate_index(sample_data)\n",
    "    print(\"Index population completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while populating the index: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding for text 'What is the main product of TechInnovators Inc.?' with dimension: 768\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Test the RAG bot with a query\u001b[39;00m\n\u001b[0;32m     10\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the main product of TechInnovators Inc.?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_qa_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[66], line 6\u001b[0m, in \u001b[0;36mrag_qa_bot\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      4\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m get_local_embedding(query)  \u001b[38;5;66;03m# Get embedding from OpenAI\u001b[39;00m\n\u001b[0;32m      5\u001b[0m similar_contexts \u001b[38;5;241m=\u001b[39m search_similar_contexts(query_embedding)\n\u001b[1;32m----> 6\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilar_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[1;32mIn[65], line 50\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[1;34m(query, contexts)\u001b[0m\n\u001b[0;32m     47\u001b[0m     prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 50\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant for a business.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[0;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1269\u001b[0m     )\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1050\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1054\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1059\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1060\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Sample function to use the RAG model\n",
    "def rag_qa_bot(query: str) -> str:\n",
    "    \"\"\"Main function to process a query using RAG.\"\"\"\n",
    "    query_embedding = get_local_embedding(query)  # Get embedding from OpenAI\n",
    "    similar_contexts = search_similar_contexts(query_embedding)\n",
    "    answer = generate_answer(query, similar_contexts)\n",
    "    return answer\n",
    "\n",
    "# Test the RAG bot with a query\n",
    "query = \"What is the main product of TechInnovators Inc.?\"\n",
    "answer = rag_qa_bot(query)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach-2 using Sentence-Transformers and PineconeDB API\n",
    "'all-mpnet-base-v2' for text to embeddings and  \"facebook/opt-1.3b\" for response Generation as 'all-mpnet-base-v2' did not give sensible responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from pinecone import Pinecone\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize Pinecone client\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "environment = \"us-east1-gcp\"\n",
    "pc = Pinecone(api_key=api_key, environment=environment)\n",
    "\n",
    "# Initialize the index\n",
    "index_name = \"task1-yardstick\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Load the Sentence-Transformers model for embeddings\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Load a more capable local language model for answer generation\n",
    "model_name = \"facebook/opt-1.3b\"  # This is a larger model, adjust based on your hardware capabilities\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "generator = pipeline(\"text-generation\", model=lm_model, tokenizer=tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embeddings using Sentence-Transformers.\"\"\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()\n",
    "\n",
    "def search_similar_contexts(query_embedding: List[float], top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Search for similar contexts in the Pinecone index.\"\"\"\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    return [{\"id\": match[\"id\"], \"score\": match[\"score\"], \"context\": match[\"metadata\"][\"text\"]} for match in results[\"matches\"]]\n",
    "\n",
    "def generate_answer_locally(query: str, contexts: List[Dict]) -> str:\n",
    "    \"\"\"Generate an answer using a local language model with retrieved contexts.\"\"\"\n",
    "    prompt = f\"Based on the following information, answer the question accurately and concisely:\\n\\n\"\n",
    "    for ctx in contexts:\n",
    "        prompt += f\"{ctx['context']}\\n\\n\"\n",
    "    prompt += f\"Question: {query}\\nAnswer:\"\n",
    "\n",
    "    # Generate response using the local model\n",
    "    response = generator(prompt, max_new_tokens=150, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
    "    \n",
    "    # Extract the generated text, removing the prompt\n",
    "    generated_text = response[0]['generated_text']\n",
    "    answer = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def rag_qa_bot(query: str) -> str:\n",
    "    \"\"\"Main function to process a query using RAG.\"\"\"\n",
    "    query_embedding = get_local_embedding(query)\n",
    "    similar_contexts = search_similar_contexts(query_embedding)\n",
    "    answer = generate_answer_locally(query, similar_contexts)\n",
    "    return answer\n",
    "\n",
    "# Function to measure accuracy using ROUGE score\n",
    "def measure_accuracy(generated_answer: str, reference_answer: str) -> Dict[str, float]:\n",
    "    \"\"\"Measure accuracy using ROUGE score.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_answer, generated_answer)\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Updating pinecone Index with larger Business related documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding for text 'Company Return and Refund Policy\n",
      "\n",
      "Section 1: General Return Policy\n",
      "Our company allows customers to return most products within 30 days of purchase, provided they are in original packaging and unused. For electronic items, the return window is 15 days, and the product must be in working condition with all accessories included.\n",
      "\n",
      "Section 2: Refund Process\n",
      "Once the returned product is received and inspected, a refund will be processed within 5-7 business days. Refunds will be credited to the original payment method. For some banks, this might take an additional 3-5 days.\n",
      "\n",
      "Section 3: Defective Product Return\n",
      "If a product is defective or damaged upon arrival, customers can request a return within 15 days. In such cases, customers have the option for a full refund, product replacement, or store credit.\n",
      "\n",
      "Section 4: Exclusions\n",
      "Some items are not eligible for return or refund, including but not limited to:\n",
      "\n",
      "Opened software\n",
      "Perishable goods\n",
      "Clearance items' with dimension: 768\n",
      "Generated embedding for text 'Product Warranty Information\n",
      "\n",
      "Section 1: Standard Warranty\n",
      "Our company provides a standard 1-year warranty on all electronic devices. This warranty covers manufacturing defects and hardware malfunctions under normal use conditions.\n",
      "\n",
      "Section 2: Extended Warranty\n",
      "Customers may purchase an extended 2-year warranty for an additional cost. This extended warranty provides coverage for repairs beyond the standard warranty period.\n",
      "\n",
      "Section 3: Claiming Warranty\n",
      "To claim warranty service, customers need to provide proof of purchase and a detailed description of the issue. Claims can be submitted via our website or by contacting customer support.\n",
      "\n",
      "Section 4: Limitations of Warranty\n",
      "The warranty does not cover damages caused by:\n",
      "\n",
      "Misuse or neglect\n",
      "Unauthorized repairs\n",
      "Accidental damage (unless covered by an extended plan)' with dimension: 768\n",
      "Generated embedding for text 'Shipping and Delivery Guidelines\n",
      "\n",
      "Section 1: Domestic Shipping\n",
      "Our company offers free standard shipping on all domestic orders over $50. For orders below $50, a shipping fee of $5 will apply. Standard shipping takes 3-5 business days.\n",
      "\n",
      "Section 2: International Shipping\n",
      "International orders may be subject to additional customs duties or taxes. Delivery times for international orders vary by location and can take up to 10-15 business days.\n",
      "\n",
      "Section 3: Express Delivery\n",
      "Customers can opt for express delivery for an additional $10. Express orders will be delivered within 1-2 business days domestically.\n",
      "\n",
      "Section 4: Delivery Issues\n",
      "If a package is lost or delayed, customers should contact customer support. We will either reship the product or issue a refund, depending on the customer's preference.' with dimension: 768\n",
      "Generated embedding for text 'Company Code of Conduct\n",
      "\n",
      "Section 1: Ethics and Integrity\n",
      "All employees are expected to maintain the highest standards of ethics and integrity in all business dealings. This includes honesty in communication, fair treatment of clients, and a commitment to ethical behavior.\n",
      "\n",
      "Section 2: Anti-Harassment Policy\n",
      "Our company has a zero-tolerance policy towards harassment or discrimination based on race, gender, religion, or any other personal characteristic. Any violations of this policy will be dealt with promptly and may result in termination.\n",
      "\n",
      "Section 3: Confidentiality\n",
      "Employees must ensure the confidentiality of all sensitive business information. Disclosure of proprietary data to unauthorized individuals is strictly prohibited.\n",
      "\n",
      "Section 4: Compliance with Laws\n",
      "All employees must comply with local, state, and federal laws. This includes adherence to employment laws, environmental regulations, and industry-specific standards.' with dimension: 768\n",
      "Generated embedding for text 'Product Specifications for \"Smart Speaker X\"\n",
      "\n",
      "Section 1: General Description\n",
      "The \"Smart Speaker X\" is a voice-activated device with a built-in AI assistant that can control home devices, play music, and answer questions. It comes with Wi-Fi and Bluetooth connectivity.\n",
      "\n",
      "Section 2: Dimensions and Weight\n",
      "\n",
      "Height: 6.5 inches\n",
      "Diameter: 4 inches\n",
      "Weight: 1.2 pounds\n",
      "Section 3: Connectivity Options\n",
      "\n",
      "Wi-Fi: 802.11 b/g/n\n",
      "Bluetooth: Version 5.0\n",
      "Voice Assistant Support: Google Assistant, Amazon Alexa\n",
      "Section 4: Battery Life\n",
      "The speaker comes with a rechargeable lithium-ion battery that provides up to 12 hours of playback on a full charge.' with dimension: 768\n",
      "Uploaded 5 vectors to Pinecone\n",
      "Index update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def update_index(documents: List[Dict[str, str]]):\n",
    "    \"\"\"Update Pinecone index with new documents.\"\"\"\n",
    "    batch_size = 100  # Adjust based on your Pinecone plan and rate limits\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        vectors = []\n",
    "        for doc in batch:\n",
    "            embedding = get_local_embedding(doc['content'])\n",
    "            vectors.append({\n",
    "                \"id\": doc['source'],\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": {\"text\": doc['content']}\n",
    "            })\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"Uploaded {len(vectors)} vectors to Pinecone\")\n",
    "\n",
    "# Prepare documents for indexing\n",
    "documents = [\n",
    "    {\"source\": \"Doc1.txt\", \"content\": \"Company Return and Refund Policy\\n\\nSection 1: General Return Policy\\nOur company allows customers to return most products within 30 days of purchase, provided they are in original packaging and unused. For electronic items, the return window is 15 days, and the product must be in working condition with all accessories included.\\n\\nSection 2: Refund Process\\nOnce the returned product is received and inspected, a refund will be processed within 5-7 business days. Refunds will be credited to the original payment method. For some banks, this might take an additional 3-5 days.\\n\\nSection 3: Defective Product Return\\nIf a product is defective or damaged upon arrival, customers can request a return within 15 days. In such cases, customers have the option for a full refund, product replacement, or store credit.\\n\\nSection 4: Exclusions\\nSome items are not eligible for return or refund, including but not limited to:\\n\\nOpened software\\nPerishable goods\\nClearance items\"},\n",
    "    {\"source\": \"Doc2.txt\", \"content\": \"Product Warranty Information\\n\\nSection 1: Standard Warranty\\nOur company provides a standard 1-year warranty on all electronic devices. This warranty covers manufacturing defects and hardware malfunctions under normal use conditions.\\n\\nSection 2: Extended Warranty\\nCustomers may purchase an extended 2-year warranty for an additional cost. This extended warranty provides coverage for repairs beyond the standard warranty period.\\n\\nSection 3: Claiming Warranty\\nTo claim warranty service, customers need to provide proof of purchase and a detailed description of the issue. Claims can be submitted via our website or by contacting customer support.\\n\\nSection 4: Limitations of Warranty\\nThe warranty does not cover damages caused by:\\n\\nMisuse or neglect\\nUnauthorized repairs\\nAccidental damage (unless covered by an extended plan)\"},\n",
    "    {\"source\": \"Doc3.txt\", \"content\": \"Shipping and Delivery Guidelines\\n\\nSection 1: Domestic Shipping\\nOur company offers free standard shipping on all domestic orders over $50. For orders below $50, a shipping fee of $5 will apply. Standard shipping takes 3-5 business days.\\n\\nSection 2: International Shipping\\nInternational orders may be subject to additional customs duties or taxes. Delivery times for international orders vary by location and can take up to 10-15 business days.\\n\\nSection 3: Express Delivery\\nCustomers can opt for express delivery for an additional $10. Express orders will be delivered within 1-2 business days domestically.\\n\\nSection 4: Delivery Issues\\nIf a package is lost or delayed, customers should contact customer support. We will either reship the product or issue a refund, depending on the customer's preference.\"},\n",
    "    {\"source\": \"Doc4.txt\", \"content\": \"Company Code of Conduct\\n\\nSection 1: Ethics and Integrity\\nAll employees are expected to maintain the highest standards of ethics and integrity in all business dealings. This includes honesty in communication, fair treatment of clients, and a commitment to ethical behavior.\\n\\nSection 2: Anti-Harassment Policy\\nOur company has a zero-tolerance policy towards harassment or discrimination based on race, gender, religion, or any other personal characteristic. Any violations of this policy will be dealt with promptly and may result in termination.\\n\\nSection 3: Confidentiality\\nEmployees must ensure the confidentiality of all sensitive business information. Disclosure of proprietary data to unauthorized individuals is strictly prohibited.\\n\\nSection 4: Compliance with Laws\\nAll employees must comply with local, state, and federal laws. This includes adherence to employment laws, environmental regulations, and industry-specific standards.\"},\n",
    "    {\"source\": \"Doc5.txt\", \"content\": \"Product Specifications for \\\"Smart Speaker X\\\"\\n\\nSection 1: General Description\\nThe \\\"Smart Speaker X\\\" is a voice-activated device with a built-in AI assistant that can control home devices, play music, and answer questions. It comes with Wi-Fi and Bluetooth connectivity.\\n\\nSection 2: Dimensions and Weight\\n\\nHeight: 6.5 inches\\nDiameter: 4 inches\\nWeight: 1.2 pounds\\nSection 3: Connectivity Options\\n\\nWi-Fi: 802.11 b/g/n\\nBluetooth: Version 5.0\\nVoice Assistant Support: Google Assistant, Amazon Alexa\\nSection 4: Battery Life\\nThe speaker comes with a rechargeable lithium-ion battery that provides up to 12 hours of playback on a full charge.\"}\n",
    "]\n",
    "\n",
    "# Update the index\n",
    "update_index(documents)\n",
    "print(\"Index update completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the return policy for electronic items?\n",
      "Generated Answer: Product Return Policy\n",
      "\n",
      "Section 1: General Return Policy\n",
      "Our company allows customers to return most products within 30 days of purchase, provided they are in original packaging and unused. For electronic items, the return window is 15 days, and the product must be in working condition with all accessories included.\n",
      "Reference Answer: For electronic items, the return window is 15 days, and the product must be in working condition with all accessories included.\n",
      "Accuracy Scores: {'rouge1': 0.5915492957746479, 'rouge2': 0.5797101449275363, 'rougeL': 0.5915492957746479}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: What does the standard warranty cover?\n",
      "Generated Answer: Maintenance and support.\n",
      "\n",
      "Maintenance and support for the original manufacturer’s warranty.\n",
      "\n",
      "The replacement of a product under this warranty, provided the product was purchased directly from our company.\n",
      "\n",
      "The repair of a product for non-warranty-related reasons, provided the product was purchased directly from our company.\n",
      "\n",
      "Reinstallation of a product under this warranty, provided the product was purchased directly from our company.\n",
      "\n",
      "The exchange of a product for a different type of product, provided the product was purchased directly from our company.\n",
      "\n",
      "The replacement of a product for a product that is the same type as the original. This includes replacement of all parts.\n",
      "\n",
      "The exchange of a product for\n",
      "Reference Answer: The standard 1-year warranty covers manufacturing defects and hardware malfunctions under normal use conditions for all electronic devices.\n",
      "Accuracy Scores: {'rouge1': 0.10687022900763359, 'rouge2': 0.0, 'rougeL': 0.07633587786259542}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: How long does domestic shipping take?\n",
      "Generated Answer: We offer free standard shipping for all domestic orders over $50. For orders below $50, a shipping fee of $5 will apply. Standard shipping takes 3-5 business days.\n",
      "\n",
      "Question: How long does international delivery take?\n",
      "Answer:\n",
      "\n",
      "International orders may be subject to additional customs duties or taxes. Delivery times for international orders vary by location and can take up to 10-15 business days.\n",
      "\n",
      "Question: How long does express delivery take?\n",
      "Answer:\n",
      "\n",
      "Customers can opt for express delivery for an additional $10. Express orders will be delivered within 1-2 business days domestically.\n",
      "\n",
      "Question: How long does custom delivery take?\n",
      "Answer:\n",
      "\n",
      "Customers can opt for custom\n",
      "Reference Answer: Standard domestic shipping takes 3-5 business days.\n",
      "Accuracy Scores: {'rouge1': 0.1391304347826087, 'rouge2': 0.08849557522123895, 'rougeL': 0.1391304347826087}\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the improved RAG bot with sample questions and measure accuracy\n",
    "sample_questions = [\n",
    "    {\n",
    "        \"question\": \"What is the return policy for electronic items?\",\n",
    "        \"reference_answer\": \"For electronic items, the return window is 15 days, and the product must be in working condition with all accessories included.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does the standard warranty cover?\",\n",
    "        \"reference_answer\": \"The standard 1-year warranty covers manufacturing defects and hardware malfunctions under normal use conditions for all electronic devices.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long does domestic shipping take?\",\n",
    "        \"reference_answer\": \"Standard domestic shipping takes 3-5 business days.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for sample in sample_questions:\n",
    "    query = sample[\"question\"]\n",
    "    reference_answer = sample[\"reference_answer\"]\n",
    "    generated_answer = rag_qa_bot(query)\n",
    "    accuracy_scores = measure_accuracy(generated_answer, reference_answer)\n",
    "    \n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "    print(f\"Reference Answer: {reference_answer}\")\n",
    "    print(f\"Accuracy Scores: {accuracy_scores}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
